#!/usr/bin/env python3
"""
AWS Batchã§è¤‡æ•°ã®ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚èµ·å‹•ã—ã¦å¤šé‡åº¦ã®å½±éŸ¿ã‚’æ¤œè¨¼ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import boto3
import json
import time
import argparse
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import uuid


class BatchJobLauncher:
    def __init__(self, job_queue, job_definition, region='us-west-2'):
        """
        Args:
            job_queue (str): AWS Batch ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼å
            job_definition (str): AWS Batch ã‚¸ãƒ§ãƒ–å®šç¾©å
            region (str): AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        """
        self.batch_client = boto3.client('batch', region_name=region)
        self.job_queue = job_queue
        self.job_definition = job_definition
        self.region = region
        
    def submit_single_job(self, job_suffix, countdown_seconds=30, job_params=None):
        """
        å˜ä¸€ã®Batchã‚¸ãƒ§ãƒ–ã‚’é€ä¿¡
        
        Args:
            job_suffix (str): ã‚¸ãƒ§ãƒ–åã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹
            countdown_seconds (int): ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ç§’æ•°
            job_params (dict): è¿½åŠ ã®ã‚¸ãƒ§ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            dict: ã‚¸ãƒ§ãƒ–é€ä¿¡çµæœ
        """
        job_name = f"concurrent-test-{job_suffix}-{int(time.time())}"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¸ãƒ§ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        default_params = {
            'jobName': job_name,
            'jobQueue': self.job_queue,
            'jobDefinition': self.job_definition,
            'parameters': {
                'countdownSeconds': str(countdown_seconds)
            }
        }
        
        # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        if job_params:
            default_params.update(job_params)
            
        try:
            start_time = datetime.now()
            response = self.batch_client.submit_job(**default_params)
            end_time = datetime.now()
            
            job_info = {
                'jobId': response['jobId'],
                'jobName': response['jobName'],
                'submissionTime': start_time.isoformat(),
                'submitDuration': (end_time - start_time).total_seconds(),
                'countdownSeconds': countdown_seconds,
                'status': 'SUBMITTED'
            }
            
            print(f"âœ“ ã‚¸ãƒ§ãƒ–é€ä¿¡å®Œäº†: {job_name} (ID: {response['jobId']})")
            return job_info
            
        except Exception as e:
            error_info = {
                'jobName': job_name,
                'error': str(e),
                'submissionTime': datetime.now().isoformat(),
                'status': 'FAILED_TO_SUBMIT'
            }
            print(f"âœ— ã‚¸ãƒ§ãƒ–é€ä¿¡å¤±æ•—: {job_name} - {str(e)}")
            return error_info
    
    def submit_concurrent_jobs(self, num_jobs, countdown_seconds=30, max_workers=10):
        """
        è¤‡æ•°ã®ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚é€ä¿¡
        
        Args:
            num_jobs (int): é€ä¿¡ã™ã‚‹ã‚¸ãƒ§ãƒ–æ•°
            countdown_seconds (int): å„ã‚¸ãƒ§ãƒ–ã®ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ç§’æ•°
            max_workers (int): åŒæ™‚å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            
        Returns:
            list: ã‚¸ãƒ§ãƒ–é€ä¿¡çµæœã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸš€ {num_jobs}å€‹ã®ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚é€ä¿¡é–‹å§‹...")
        print(f"   ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼: {self.job_queue}")
        print(f"   ã‚¸ãƒ§ãƒ–å®šç¾©: {self.job_definition}")
        print(f"   ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³: {countdown_seconds}ç§’")
        print(f"   æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        print("-" * 50)
        
        start_time = datetime.now()
        job_results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å…¨ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚é€ä¿¡
            future_to_job = {
                executor.submit(self.submit_single_job, f"job{i:03d}", countdown_seconds): i 
                for i in range(1, num_jobs + 1)
            }
            
            # çµæœã‚’åé›†
            for future in as_completed(future_to_job):
                job_result = future.result()
                job_results.append(job_result)
        
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        print("-" * 50)
        print(f"ğŸ“Š é€ä¿¡å®Œäº†: {len(job_results)}å€‹ã®ã‚¸ãƒ§ãƒ–")
        print(f"   ç·é€ä¿¡æ™‚é–“: {total_duration:.2f}ç§’")
        print(f"   å¹³å‡é€ä¿¡æ™‚é–“: {total_duration/len(job_results):.2f}ç§’/ã‚¸ãƒ§ãƒ–")
        
        # é€ä¿¡æˆåŠŸãƒ»å¤±æ•—ã®çµ±è¨ˆ
        successful = [j for j in job_results if j['status'] == 'SUBMITTED']
        failed = [j for j in job_results if j['status'] == 'FAILED_TO_SUBMIT']
        
        print(f"   æˆåŠŸ: {len(successful)}å€‹")
        print(f"   å¤±æ•—: {len(failed)}å€‹")
        
        return job_results
    
    def monitor_jobs(self, job_results, check_interval=10):
        """
        é€ä¿¡ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’ç›£è¦–
        
        Args:
            job_results (list): submit_concurrent_jobsã®æˆ»ã‚Šå€¤
            check_interval (int): ãƒã‚§ãƒƒã‚¯é–“éš”ï¼ˆç§’ï¼‰
        """
        successful_jobs = [j for j in job_results if j['status'] == 'SUBMITTED']
        if not successful_jobs:
            print("ç›£è¦–å¯¾è±¡ã®ã‚¸ãƒ§ãƒ–ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        job_ids = [j['jobId'] for j in successful_jobs]
        print(f"ğŸ“ˆ {len(job_ids)}å€‹ã®ã‚¸ãƒ§ãƒ–ã‚’ç›£è¦–é–‹å§‹...")
        
        completed_jobs = set()
        
        while len(completed_jobs) < len(job_ids):
            try:
                # ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã‚’å–å¾—
                response = self.batch_client.describe_jobs(jobs=job_ids)
                
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"\n[{current_time}] ã‚¸ãƒ§ãƒ–çŠ¶æ…‹:")
                
                status_count = {}
                
                for job in response['jobs']:
                    job_id = job['jobId']
                    job_name = job['jobName']
                    status = job['jobStatus']
                    
                    # çŠ¶æ…‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    status_count[status] = status_count.get(status, 0) + 1
                    
                    # å®Œäº†ã—ãŸã‚¸ãƒ§ãƒ–ã‚’è¨˜éŒ²
                    if status in ['SUCCEEDED', 'FAILED'] and job_id not in completed_jobs:
                        completed_jobs.add(job_id)
                        
                        if status == 'SUCCEEDED':
                            print(f"  âœ“ {job_name}: {status}")
                        else:
                            print(f"  âœ— {job_name}: {status}")
                            if 'statusReason' in job:
                                print(f"    ç†ç”±: {job['statusReason']}")
                    elif status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']:
                        print(f"  â³ {job_name}: {status}")
                
                # çŠ¶æ…‹ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                print(f"  çŠ¶æ…‹ã‚µãƒãƒªãƒ¼: {dict(status_count)}")
                print(f"  å®Œäº†: {len(completed_jobs)}/{len(job_ids)}")
                
                if len(completed_jobs) < len(job_ids):
                    time.sleep(check_interval)
                    
            except Exception as e:
                print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {str(e)}")
                time.sleep(check_interval)
        
        print("\nğŸ‰ å…¨ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
    def save_results(self, job_results, output_file):
        """
        çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            job_results (list): ã‚¸ãƒ§ãƒ–é€ä¿¡çµæœ
            output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        result_data = {
            'timestamp': datetime.now().isoformat(),
            'jobQueue': self.job_queue,
            'jobDefinition': self.job_definition,
            'totalJobs': len(job_results),
            'successfulJobs': len([j for j in job_results if j['status'] == 'SUBMITTED']),
            'failedJobs': len([j for j in job_results if j['status'] == 'FAILED_TO_SUBMIT']),
            'jobs': job_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='AWS Batchã‚¸ãƒ§ãƒ–åŒæ™‚èµ·å‹•ãƒ†ã‚¹ãƒˆ')
    parser.add_argument('--job-queue', required=True, help='Batchã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼å')
    parser.add_argument('--job-definition', required=True, help='Batchã‚¸ãƒ§ãƒ–å®šç¾©å')
    parser.add_argument('--num-jobs', type=int, default=5, help='èµ·å‹•ã™ã‚‹ã‚¸ãƒ§ãƒ–æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    parser.add_argument('--countdown', type=int, default=30, help='ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ç§’æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30)')
    parser.add_argument('--max-workers', type=int, default=10, help='æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)')
    parser.add_argument('--region', default='us-west-2', help='AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: us-west-2)')
    parser.add_argument('--output', help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (JSON)')
    parser.add_argument('--monitor', action='store_true', help='ã‚¸ãƒ§ãƒ–å®Ÿè¡Œã‚’ç›£è¦–ã™ã‚‹')
    parser.add_argument('--monitor-interval', type=int, default=10, help='ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    # Batch Job Launcherã‚’åˆæœŸåŒ–
    launcher = BatchJobLauncher(
        job_queue=args.job_queue,
        job_definition=args.job_definition,
        region=args.region
    )
    
    # ã‚¸ãƒ§ãƒ–ã‚’åŒæ™‚é€ä¿¡
    job_results = launcher.submit_concurrent_jobs(
        num_jobs=args.num_jobs,
        countdown_seconds=args.countdown,
        max_workers=args.max_workers
    )
    
    # çµæœã‚’ä¿å­˜
    if args.output:
        launcher.save_results(job_results, args.output)
    
    # ç›£è¦–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if args.monitor:
        launcher.monitor_jobs(job_results, args.monitor_interval)


if __name__ == "__main__":
    main()
