#!/usr/bin/env python3
"""
AWS Batch å¤šé‡åº¦ãƒ†ã‚¹ãƒˆã®çµæœã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import os
import sys
import glob
from datetime import datetime
import statistics

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªä¾å­˜é–¢ä¿‚
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False


def load_test_results(results_dir):
    """
    ãƒ†ã‚¹ãƒˆçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        results_dir (str): çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        
    Returns:
        list: ãƒ†ã‚¹ãƒˆçµæœã®ãƒªã‚¹ãƒˆ
    """
    results = []
    json_files = glob.glob(os.path.join(results_dir, "*.json"))
    
    for file_path in sorted(json_files):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                data['filename'] = os.path.basename(file_path)
                results.append(data)
        except Exception as e:
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    return results


def analyze_submission_performance(results):
    """
    ã‚¸ãƒ§ãƒ–é€ä¿¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ
    
    Args:
        results (list): ãƒ†ã‚¹ãƒˆçµæœã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: åˆ†æçµæœ
    """
    analysis = {}
    
    for result in results:
        test_name = result['filename'].replace('.json', '')
        
        successful_jobs = [j for j in result['jobs'] if j['status'] == 'SUBMITTED']
        submit_durations = [j.get('submitDuration', 0) for j in successful_jobs]
        
        if submit_durations:
            analysis[test_name] = {
                'total_jobs': result['totalJobs'],
                'successful_jobs': len(successful_jobs),
                'failed_jobs': result['failedJobs'],
                'success_rate': len(successful_jobs) / result['totalJobs'] * 100,
                'avg_submit_time': statistics.mean(submit_durations),
                'median_submit_time': statistics.median(submit_durations),
                'max_submit_time': max(submit_durations),
                'min_submit_time': min(submit_durations),
                'std_submit_time': statistics.stdev(submit_durations) if len(submit_durations) > 1 else 0
            }
    
    return analysis


def generate_performance_report(analysis, output_file):
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        analysis (dict): åˆ†æçµæœ
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    report_lines = [
        "# AWS Batch å¤šé‡åº¦ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ",
        f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼",
        ""
    ]
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
    report_lines.extend([
        "| ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ | ã‚¸ãƒ§ãƒ–æ•° | æˆåŠŸç‡ | å¹³å‡é€ä¿¡æ™‚é–“ | ä¸­å¤®å€¤é€ä¿¡æ™‚é–“ | æœ€å¤§é€ä¿¡æ™‚é–“ | æ¨™æº–åå·® |",
        "|-------------|----------|--------|-------------|---------------|-------------|----------|"
    ])
    
    # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¿½åŠ 
    for test_name, data in analysis.items():
        row = (
            f"| {test_name} | {data['total_jobs']} | "
            f"{data['success_rate']:.1f}% | "
            f"{data['avg_submit_time']:.3f}s | "
            f"{data['median_submit_time']:.3f}s | "
            f"{data['max_submit_time']:.3f}s | "
            f"{data['std_submit_time']:.3f}s |"
        )
        report_lines.append(row)
    
    report_lines.extend([
        "",
        "## è©³ç´°åˆ†æ",
        ""
    ])
    
    # è©³ç´°åˆ†æ
    for test_name, data in analysis.items():
        report_lines.extend([
            f"### {test_name}",
            f"- **ç·ã‚¸ãƒ§ãƒ–æ•°**: {data['total_jobs']}",
            f"- **æˆåŠŸã‚¸ãƒ§ãƒ–æ•°**: {data['successful_jobs']}",
            f"- **å¤±æ•—ã‚¸ãƒ§ãƒ–æ•°**: {data['failed_jobs']}",
            f"- **æˆåŠŸç‡**: {data['success_rate']:.1f}%",
            f"- **å¹³å‡é€ä¿¡æ™‚é–“**: {data['avg_submit_time']:.3f}ç§’",
            f"- **é€ä¿¡æ™‚é–“ä¸­å¤®å€¤**: {data['median_submit_time']:.3f}ç§’",
            f"- **æœ€å¤§é€ä¿¡æ™‚é–“**: {data['max_submit_time']:.3f}ç§’",
            f"- **æœ€å°é€ä¿¡æ™‚é–“**: {data['min_submit_time']:.3f}ç§’",
            f"- **æ¨™æº–åå·®**: {data['std_submit_time']:.3f}ç§’",
            ""
        ])
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘ã®åˆ†æ
    job_counts = [data['total_jobs'] for data in analysis.values()]
    avg_times = [data['avg_submit_time'] for data in analysis.values()]
    
    if len(job_counts) > 1:
        report_lines.extend([
            "## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘",
            "",
            "### å¤šé‡åº¦ã«ã‚ˆã‚‹å½±éŸ¿ã®è©•ä¾¡:",
            ""
        ])
        
        # æœ€å°ã¨æœ€å¤§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ
        min_jobs_idx = job_counts.index(min(job_counts))
        max_jobs_idx = job_counts.index(max(job_counts))
        
        min_jobs_data = list(analysis.values())[min_jobs_idx]
        max_jobs_data = list(analysis.values())[max_jobs_idx]
        
        performance_ratio = max_jobs_data['avg_submit_time'] / min_jobs_data['avg_submit_time']
        
        report_lines.extend([
            f"- **æœ€å°å¤šé‡åº¦** ({min_jobs_data['total_jobs']}ã‚¸ãƒ§ãƒ–): {min_jobs_data['avg_submit_time']:.3f}ç§’",
            f"- **æœ€å¤§å¤šé‡åº¦** ({max_jobs_data['total_jobs']}ã‚¸ãƒ§ãƒ–): {max_jobs_data['avg_submit_time']:.3f}ç§’",
            f"- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”**: {performance_ratio:.2f}x",
            ""
        ])
        
        if performance_ratio > 1.5:
            report_lines.append("âš ï¸ **è­¦å‘Š**: é«˜ã„å¤šé‡åº¦ã«ã‚ˆã‚Šé€ä¿¡æ™‚é–“ãŒ50%ä»¥ä¸Šå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚")
        elif performance_ratio > 1.2:
            report_lines.append("âš¡ **æ³¨æ„**: å¤šé‡åº¦ã«ã‚ˆã‚Šé€ä¿¡æ™‚é–“ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ãŒã€è¨±å®¹ç¯„å›²å†…ã§ã™ã€‚")
        else:
            report_lines.append("âœ… **è‰¯å¥½**: å¤šé‡åº¦ã«ã‚ˆã‚‹å¤§ããªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")


def create_performance_charts(analysis, output_dir):
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ
    
    Args:
        analysis (dict): åˆ†æçµæœ
        output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    if not HAS_MATPLOTLIB:
        print("âš ï¸  matplotlib ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒãƒ£ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
        
    try:
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        test_names = list(analysis.keys())
        job_counts = [analysis[name]['total_jobs'] for name in test_names]
        avg_times = [analysis[name]['avg_submit_time'] for name in test_names]
        success_rates = [analysis[name]['success_rate'] for name in test_names]
        
        # å›³1: ã‚¸ãƒ§ãƒ–æ•° vs å¹³å‡é€ä¿¡æ™‚é–“
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.plot(job_counts, avg_times, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('ã‚¸ãƒ§ãƒ–æ•°')
        plt.ylabel('å¹³å‡é€ä¿¡æ™‚é–“ (ç§’)')
        plt.title('ã‚¸ãƒ§ãƒ–æ•° vs å¹³å‡é€ä¿¡æ™‚é–“')
        plt.grid(True, alpha=0.3)
        
        # å›³2: ã‚¸ãƒ§ãƒ–æ•° vs æˆåŠŸç‡
        plt.subplot(2, 2, 2)
        plt.plot(job_counts, success_rates, 'go-', linewidth=2, markersize=8)
        plt.xlabel('ã‚¸ãƒ§ãƒ–æ•°')
        plt.ylabel('æˆåŠŸç‡ (%)')
        plt.title('ã‚¸ãƒ§ãƒ–æ•° vs æˆåŠŸç‡')
        plt.ylim(0, 105)
        plt.grid(True, alpha=0.3)
        
        # å›³3: é€ä¿¡æ™‚é–“ã®åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        plt.bar(range(len(test_names)), avg_times, color='skyblue', alpha=0.7)
        plt.xlabel('ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹')
        plt.ylabel('å¹³å‡é€ä¿¡æ™‚é–“ (ç§’)')
        plt.title('ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åˆ¥ å¹³å‡é€ä¿¡æ™‚é–“')
        plt.xticks(range(len(test_names)), [name.split('-')[-1] for name in test_names], rotation=45)
        plt.grid(True, alpha=0.3)
        
        # å›³4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ¹ç‡
        plt.subplot(2, 2, 4)
        efficiency = [job_counts[i] / avg_times[i] for i in range(len(job_counts))]
        plt.plot(job_counts, efficiency, 'ro-', linewidth=2, markersize=8)
        plt.xlabel('ã‚¸ãƒ§ãƒ–æ•°')
        plt.ylabel('åŠ¹ç‡ (ã‚¸ãƒ§ãƒ–/ç§’)')
        plt.title('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆåŠ¹ç‡')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        chart_file = os.path.join(output_dir, 'performance-charts.png')
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {chart_file}")
        
    except ImportError:
        print("âš ï¸  matplotlib ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒãƒ£ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    except Exception as e:
        print(f"âš ï¸  ãƒãƒ£ãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")


def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ³•: python3 analyze-test-results.py <results_directory>")
        sys.exit(1)
    
    results_dir = sys.argv[1]
    
    if not os.path.exists(results_dir):
        print(f"âŒ çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_dir}")
        sys.exit(1)
    
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æä¸­: {results_dir}")
    
    # ãƒ†ã‚¹ãƒˆçµæœã‚’èª­ã¿è¾¼ã¿
    results = load_test_results(results_dir)
    
    if not results:
        print("âŒ åˆ†æå¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    print(f"ğŸ“„ {len(results)}å€‹ã®ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ
    analysis = analyze_submission_performance(results)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report_file = os.path.join(results_dir, 'performance-report.md')
    generate_performance_report(analysis, report_file)
    
    # ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ
    create_performance_charts(analysis, results_dir)
    
    print("\nğŸ‰ åˆ†æå®Œäº†ï¼")
    print(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
    print(f"   ãƒãƒ£ãƒ¼ãƒˆ: {os.path.join(results_dir, 'performance-charts.png')}")


if __name__ == "__main__":
    main()
